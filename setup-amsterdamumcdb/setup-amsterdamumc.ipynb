{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/logo_amds.png\" alt=\"Logo\" style=\"width: 128px;\"/>\n",
    "\n",
    "# AmsterdamUMCdb - Freely Accessible ICU Database\n",
    "\n",
    "version 1.0.1 January 2020  \n",
    "Copyright &copy; 2003-2020 Amsterdam UMC - Amsterdam Medical Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup AmsterdamUMCdb\n",
    "## Requirements\n",
    "- Access to the AmsterdamUMCdb csv files: request access from [Amsterdam Medical Data Science](https://www.amsterdammedicaldatascience.nl/).\n",
    "- Operating system: any OS capable of running Python and PostgreSQL, including Windows, macOS and Linux.\n",
    "- Internal memory: 8GB should suffice for basic analysis and running the Jupyter notebooks. However, the recommended memory specification to run both PostgreSQL and the Jupyter Notebooks on the same machine is 16-32 GB.\n",
    "- Disk space: Downloading and extracting the database files will require 90 GB of hard disk space. In addition, creating the PostgreSQL database requires about 128 GB of hard disk space and and an additional 144 GB for creating the indices to improve query performance. \n",
    "\n",
    "## 1. Install a Python distribution\n",
    "We **strongly recommend** installing Python using Anaconda, a popular distribution that includes many useful modules for data science out-of-the-box. Install the (latest) Python 3.7 version distribution from [Anaconda's](https://www.anaconda.com/distribution) distribution page.\n",
    "\n",
    "## 2. Install PostgreSQL\n",
    "PostgreSQL is an open source database management system (DBMS), available for most operating systems, including Windows, macOS and Linux. We recommend the installation of the most recent version of PostgreSQL (version 12) from the PostgreSQL [download](https://www.postgresql.org/download/) page. Please note your password for the `postgres` superuser, and if you did not chose `postgres` as the password, you need to modify these settings in the [`config.SAMPLE.ini`](../config.SAMPLE.ini) file in the root of the repository. Save the file as [`config.ini`](../config.ini).\n",
    "\n",
    "## 3. Install psycopg2 module\n",
    "To connect to your postgreSQL server from Python, the [psycopg2](https://pypi.org/project/psycopg2/) package needs to be installed from the Anaconda Prompt/Shell using conda:\n",
    "\n",
    "> conda install -c anaconda psycopg2\n",
    "\n",
    "## 4. Clone the AmsterdamUMCdb GitHub respository\n",
    "Clone or download the [AmsterdamUMCdb](https://github.com/AmsterdamUMC/AmsterdamUMCdb) repository from GitHub. \n",
    "Follow the instructions on GitHub's online step-by-step guide, if needed: https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository. \n",
    "\n",
    "## 5. Download the database files\n",
    "Download the AmsterdamUMCdb zip file from and extract the files from the zip file to the [`data`](../data) folder of the cloned AmsterdamUMCdb repository.\n",
    "\n",
    "## 6. Create database tables\n",
    "Start the Jupyter notebook server from the command line (using Command Prompt on Windows or Terminal on Mac/Linux) by running\n",
    "\n",
    "> jupyter notebook\n",
    "\n",
    "From the Jupyter file browser, open the `setup-amsterdamumc.ipynb` file from the `setup-amsterdamumc` folder in the cloned repository. This code in the notebook assumes there is a default postgres installation with a dabase named `postgres`, user `postgres` with password `postgres`. You should change these settings in the [`config.SAMPLE.ini`](../config.SAMPLE.ini) file in the root of the repository and save the file as [`config.ini`](../config.ini).\n",
    "To create the tables in the database run this Jupyter notebook, either cell by cell (▶️ Run) or use the ⏩ button to  perform all steps sequentially. An `amsterdamumc` [schema](https://www.postgresql.org/docs/12/ddl-schemas.html) will be created, and all tables will be added to this schema.\n",
    "\n",
    "## 7. Verify the database\n",
    "After this notebook has been run completely, the postgres database should contain all tables with the same number of records we released. The output should state `Verification: PASSED`. You can verify it [here](#verify).\n",
    " \n",
    "## 8. Create database table indices\n",
    "It's highly recommended to create some useful indices to improve performance for common queries on identifiers like admissionid, itemid and measured times. \n",
    "\n",
    "## 9. Jupyter Notebooks\n",
    "While the indices are being created, the postgreSQL should be available for querying using the notebooks in the [`tables`](../tables) folder (with lower performance). We use  plotly (version >4) for interactive plots in some notebooks. Plotly can be installed by using conda:\n",
    "\n",
    "> conda install -c plotly plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python settings\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import os\n",
    "from IPython.display import display, HTML, Markdown, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "class ProgressFile:\n",
    "    \"\"\"ProgressFile: simple class to work as a file-like object that can be used by function/library's to read the file\n",
    "    while at the same time updating a progress bar.\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, pbar, filename, mode=\"r\"):\n",
    "        self.file = open(filename, mode, encoding='windows-1252')\n",
    "        self.progress = self.file.tell()\n",
    "        self.pbar = pbar\n",
    "        self.file.readline() #skip the first line: header\n",
    "        \n",
    "        #uncomment when using v1.0.0 (Microsoft SQL server) tsv/csv files:\n",
    "        #self.file.readline() #skip the second line: \"-----------\" from MS SQL csv export\n",
    "\n",
    "    def close(self):\n",
    "        self.file.seek(0, os.SEEK_END)\n",
    "        self.pbar.n = self.file.tell()\n",
    "        self.pbar.refresh()\n",
    "        self.file.close()\n",
    "        \n",
    "    def read(self, size):\n",
    "        buf = self.file.read(size)\n",
    "        self.pbar.update(size)\n",
    "        \n",
    "        # backslash is considered an escape character and not allowed in postgresql in COPY FROM statement\n",
    "        # replace with double-backslash.\n",
    "        \n",
    "        #uncomment when using v1.0.0 (Microsoft SQL server) tsv/csv files:\n",
    "        #return buf.replace('\\\\','\\\\\\\\')\n",
    "        \n",
    "        #comment when using v1.0.0 (Microsoft SQL server) tsv/csv files:\n",
    "        return buf\n",
    "        \n",
    "    def readline(self, size):\n",
    "        buf = self.file.readline(size)\n",
    "        self.pbar.update(size)\n",
    "        \n",
    "        # backslash is considered an escape character and not allowed in postgresql in COPY FROM statement\n",
    "        # replace with double-backslash.\n",
    "        \n",
    "        #uncomment when using v1.0.0 (Microsoft SQL server) tsv/csv files:\n",
    "        #return buf.replace('\\\\','\\\\\\\\')\n",
    "        \n",
    "        #comment when using v1.0.0 (Microsoft SQL server) tsv/csv files:\n",
    "        return buf\n",
    "    \n",
    "    \n",
    "def copy_progress(csv, table):\n",
    "    \"\"\"copy the csv file to the table using tdqm progressbar.\n",
    "    \"\"\"\n",
    "    #import the database using a tqdm progressbar and a ProgressFile\n",
    "    pbar = tqdm(total=os.path.getsize(csv), desc='Importing '+os.path.splitext(os.path.basename(csv))[0],\n",
    "            dynamic_ncols=True, unit_scale=1, unit='Bytes') #make a tdqm progress bar object\n",
    "\n",
    "    pfile = ProgressFile(pbar, csv, 'r') #create a ProgressFile for showing progress\n",
    "    \n",
    "    #uncomment when using v1.0.0 (Microsoft SQL server) tsv/csv files:\n",
    "    #cursor.copy_from(pfile, table, null=\"NULL\")\n",
    "    \n",
    "    #comment when using v1.0.0 (Microsoft SQL server) tsv/csv files:\n",
    "    cursor.copy_expert(\"\"\"COPY {} FROM STDIN WITH (FORMAT CSV)\"\"\".format(table), pfile)\n",
    "        \n",
    "    #close the objects\n",
    "    pfile.close()\n",
    "    pbar.close()\n",
    "\n",
    "    #show the first 10 records of this table\n",
    "    df = pd.read_sql('SELECT * FROM ' + table + ' LIMIT 10',con)\n",
    "    display(Markdown('## ' + os.path.splitext(os.path.basename(csv))[0] + ' (' + str(cursor.rowcount) + ' records copied):\\n'))\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify config.ini in the root folder of the repository to change the settings to connect to your postgreSQL database\n",
    "import configparser\n",
    "import os\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "if os.path.isfile('../config.ini'):\n",
    "    config.read('../config.ini')\n",
    "else:\n",
    "    config.read('../config.SAMPLE.ini')\n",
    "\n",
    "#Open a connection to the postgres database:\n",
    "con = psycopg2.connect(database=config['psycopg2']['database'], \n",
    "                       user=config['psycopg2']['username'], password=config['psycopg2']['password'], \n",
    "                       host=config['psycopg2']['host'], port=config['psycopg2']['port'])\n",
    "con.set_client_encoding('WIN1252') #Uses code page for Dutch accented characters.\n",
    "con.set_session(autocommit=True)\n",
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create schema\n",
    "Create an `amsterdamumc` schema to prevent collissions with (possible) other tables in the default `public` schema and change the schema path to our newly created 'amsterdamumc' schema to access the data without schema qualifications (e.g. `admissions` instead of `amsterdamumc.admissions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS amsterdamumcdb;\n",
    "GRANT USAGE ON SCHEMA amsterdamumcdb TO public;\n",
    "GRANT CREATE ON SCHEMA amsterdamumcdb TO public;\n",
    "SET SCHEMA 'amsterdamumcdb';\n",
    "\"\"\"\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create admissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "af0db8dd-df4e-4305-b2ac-f10e8fefe9b4"
   },
   "outputs": [],
   "source": [
    "table = 'admissions'\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS admissions CASCADE;\n",
    "CREATE TABLE admissions \n",
    "(\n",
    "    patientid INTEGER,\n",
    "    admissionid serial PRIMARY KEY,\n",
    "    admissioncount INTEGER,\n",
    "    location VARCHAR,\n",
    "    urgency BIT,\n",
    "    origin VARCHAR,\n",
    "    admittedat BIGINT,\n",
    "    admissionyeargroup VARCHAR,\n",
    "    dischargedat BIGINT,\n",
    "    lengthofstay SMALLINT,\n",
    "    destination VARCHAR,\n",
    "    gender VARCHAR,\n",
    "    agegroup VARCHAR,\n",
    "    dateofdeath BIGINT,\n",
    "    weightgroup VARCHAR,\n",
    "    weightsource VARCHAR,\n",
    "    heightgroup VARCHAR,\n",
    "    heightsource VARCHAR,\n",
    "    specialty VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "csv = os.path.join('..', config['files']['datapath'], config['files'][table])\n",
    "copy_progress(csv, table) #runs copy_from using a tdqm progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create drugitems table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "af0db8dd-df4e-4305-b2ac-f10e8fefe9b4"
   },
   "outputs": [],
   "source": [
    "table = 'drugitems'\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS drugitems CASCADE;\n",
    "CREATE TABLE drugitems \n",
    "(\n",
    "    admissionid INTEGER,\n",
    "    orderid BIGINT,\n",
    "    ordercategoryid INTEGER,\n",
    "    ordercategory VARCHAR,\n",
    "    itemid INTEGER,\n",
    "    item VARCHAR,\n",
    "    isadditive BIT,\n",
    "    isconditional BIT,\n",
    "    rate FLOAT,\n",
    "    rateunit VARCHAR,\n",
    "    rateunitid INTEGER,\n",
    "    ratetimeunitid INTEGER,\n",
    "    doserateperkg BIT,\n",
    "    dose FLOAT,\n",
    "    doseunit VARCHAR,\n",
    "    doserateunit VARCHAR,\n",
    "    doseunitid INTEGER,    \n",
    "    doserateunitid INTEGER,\n",
    "    administered FLOAT,\n",
    "    administeredunit VARCHAR,\n",
    "    administeredunitid INTEGER,\n",
    "    action VARCHAR,\n",
    "    start BIGINT,\n",
    "    stop BIGINT,\n",
    "    duration BIGINT,\n",
    "    solutionitemid INTEGER,\n",
    "    solutionitem VARCHAR,\n",
    "    solutionadministered FLOAT,\n",
    "    solutionadministeredunit VARCHAR,\n",
    "    fluidin FLOAT,\n",
    "    iscontinuous BIT\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "csv = os.path.join('..', config['files']['datapath'], config['files'][table])\n",
    "copy_progress(csv, table) #runs copy_from using a tdqm progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create freetextitems table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'freetextitems'\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS freetextitems CASCADE;\n",
    "CREATE TABLE freetextitems \n",
    "(\n",
    "    admissionid INTEGER,\n",
    "    itemid BIGINT,\n",
    "    item VARCHAR,\n",
    "    value VARCHAR,\n",
    "    comment VARCHAR,\n",
    "    measuredat BIGINT,\n",
    "    registeredat BIGINT,\n",
    "    registeredby VARCHAR,\n",
    "    updatedat BIGINT,\n",
    "    updatedby VARCHAR,\n",
    "    islabresult BIT\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "csv = os.path.join('..', config['files']['datapath'], config['files'][table])\n",
    "copy_progress(csv, table) #runs copy_from using a tdqm progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create listitems table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'listitems'\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS listitems CASCADE;\n",
    "CREATE TABLE listitems \n",
    "(\n",
    "    admissionid INTEGER,\n",
    "    itemid BIGINT,\n",
    "    item VARCHAR,\n",
    "    valueid INT,\n",
    "    value VARCHAR,\n",
    "    measuredat BIGINT,\n",
    "    registeredat BIGINT,\n",
    "    registeredby VARCHAR,\n",
    "    updatedat BIGINT,\n",
    "    updatedby VARCHAR,\n",
    "    islabresult BIT\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "csv = os.path.join('..', config['files']['datapath'], config['files'][table])\n",
    "copy_progress(csv, table) #runs copy_from using a tdqm progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create numericitems table\n",
    "This is the largest table and can take a while depending on the performance of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'numericitems'\n",
    "sql = \"\"\"\n",
    "    DROP TABLE IF EXISTS numericitems CASCADE;\n",
    "    CREATE TABLE numericitems \n",
    "    (\n",
    "    admissionid INTEGER,\n",
    "    itemid BIGINT,\n",
    "    item VARCHAR,\n",
    "    tag VARCHAR,\n",
    "    value FLOAT,\n",
    "    unitid INT,\n",
    "    unit VARCHAR,\n",
    "    comment VARCHAR,\n",
    "    measuredat BIGINT,\n",
    "    registeredat BIGINT,\n",
    "    registeredby VARCHAR,\n",
    "    updatedat BIGINT,\n",
    "    updatedby VARCHAR,\n",
    "    islabresult BIT,\n",
    "    fluidout FLOAT\n",
    "    )\n",
    "\"\"\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "csv = os.path.join('..', config['files']['datapath'], config['files'][table])\n",
    "copy_progress(csv, table) #runs copy_from using a tdqm progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixes a conversion error in the master database from Fahrenheit to Celsius\n",
    "<b>TO DO</b>: fix the master database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixes a conversion error in the master numericitems database from Fahrenheit to Celsius\n",
    "sql_fix_temperature = \"\"\"\n",
    "UPDATE numericitems\n",
    "SET value = ROUND((((value * 1.8) - 32 )/1.8)::numeric, 1) --updates the values with the correct temperature in Celsius\n",
    "WHERE unitid = 59 --degrees Celsius, originally stored in the MetaVision PDMS in degrees Fahrenheit\n",
    "\"\"\"\n",
    "cursor.execute(sql_fix_temperature)\n",
    "print(\"Number of rows UPDATEd: \" + str(cursor.rowcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create procedureorderitems table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'procedureorderitems'\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS procedureorderitems CASCADE;\n",
    "CREATE TABLE procedureorderitems \n",
    "(\n",
    "    admissionid INTEGER,\n",
    "    orderid BIGINT,\n",
    "    ordercategoryid INT,\n",
    "    ordercategoryname VARCHAR,\n",
    "    itemid INT,\n",
    "    item VARCHAR,\n",
    "    registeredat BIGINT,\n",
    "    registeredby VARCHAR\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "csv = os.path.join('..', config['files']['datapath'], config['files'][table])\n",
    "copy_progress(csv, table) #runs copy_from using a tdqm progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create processitems table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'processitems'\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS processitems CASCADE;\n",
    "CREATE TABLE processitems \n",
    "(\n",
    "    admissionid INTEGER,\n",
    "    itemid BIGINT,\n",
    "    item VARCHAR,\n",
    "    start BIGINT,\n",
    "    stop BIGINT,\n",
    "    duration BIGINT\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "csv = os.path.join('..', config['files']['datapath'], config['files'][table])\n",
    "copy_progress(csv, table) #runs copy_from using a tdqm progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='verify'></a>Verify record counts with published data\n",
    "Compares the counts of the imported tables with our published number of records to verify that they match. Since importing in postgreSQL is already very strict using the COPY FROM command (same number of rows, compatible datatypes), an equal number of rows assumes a correct import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AmsterdamUMCdb Version 1.0.1 record counts\n",
    "data = [\n",
    "['admissions', 23172],\n",
    "['drugitems', 4931782],\n",
    "['freetextitems', 653040],\n",
    "['listitems', 31072879 ],\n",
    "['numericitems', 978581796],\n",
    "['procedureorderitems', 2196116],\n",
    "['processitems', 257740]]\n",
    "\n",
    "counts_published = pd.DataFrame(data, columns=['tables', 'counts'])\n",
    "counts_published = counts_published.set_index('tables')\n",
    "\n",
    "failed = False\n",
    "html = u'<table style=\"font-size:16px\" ><th style=\"text-align:left\">Table<th>Counts postgres<th>Counts published<th style=\"text-align:center\">Verified'\n",
    "for table in counts_published.index:\n",
    "        sql = \"SELECT COUNT(admissionid) FROM \" + table + \";\"\n",
    "        try:\n",
    "            cursor.execute(sql)\n",
    "            count = cursor.fetchone()[0]\n",
    "        except:\n",
    "            count = 0\n",
    "        \n",
    "        count_published = counts_published.loc[table, 'counts']\n",
    "        \n",
    "        if count == count_published:\n",
    "            count_html = str(count)\n",
    "            image = '<svg viewBox=\"0 0 12 16\" version=\"1.1\" width=\"24\" height=\"32\" aria-hidden=\"true\">\\\n",
    "            <path fill=\"#28a745\" fill-rule=\"evenodd\" d=\"M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z\"></path></svg>'\n",
    "        else:\n",
    "            failed = True\n",
    "            count_html = '<font color=\"#cb2431\"><b>' + str(count) + '</b></font>'\n",
    "            image = '<svg viewBox=\"0 0 12 16\" version=\"1.1\" width=\"24\" height=\"32\" aria-hidden=\"true\">\\\n",
    "            <path fill=\"#cb2431\" fill-rule=\"evenodd\" d=\"M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 \\\n",
    "            8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z\"></path></svg>'\n",
    "        \n",
    "        html = html + '<tr><td style=\"text-align:left\" width=\"256\">' + table + '</td><td width=\"256\">' + count_html + '</td><td width=\"256\">' + str(count_published) + \\\n",
    "            '</td><td width=\"256\" style=\"text-align:center\">' + image + '</td></tr>'\n",
    "        clear_output(wait=True)\n",
    "        get_ipython().run_cell_magic(u'HTML', u'', html)\n",
    "\n",
    "if failed:\n",
    "    conclusion = '<tr></tr><tr bgcolor=\"#cb2431\"><td style=\"text-align:left\"><font style=\"font-size:30px\" color=\"#ffffff\"><b>Verification:</b></font></td><td></td><td></td><td><font style=\"font-size:30px\" color=\"#ffffff\"><b>FAILED!</b></font></td>'\n",
    "else:\n",
    "    conclusion = '<tr></tr><tr bgcolor=\"#28a745\"><td style=\"text-align:left\"><font style=\"font-size:30px\" color=\"#ffffff\"><b>Verification:</b></td></font><td></td><td></td><td><font style=\"font-size:30px\" color=\"#ffffff\"><b>PASSED</b></font></td>'\n",
    "\n",
    "clear_output(wait=True)\n",
    "html = html + conclusion + '</td></tr></table>'\n",
    "get_ipython().run_cell_magic(u'HTML', u'', html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Indices to increase performance\n",
    "After verification this will run to create indices to improve query performance. However, this is a height process that can take hours depending on your system. In the meantime, the database is already to be queried (albeit slower), using the notebooks from the [tables](../tables/) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import select\n",
    "import psycopg2.extensions\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "def wait(conn):\n",
    "    while True:\n",
    "        state = conn.poll()\n",
    "        if state == psycopg2.extensions.POLL_OK:\n",
    "            break\n",
    "        elif state == psycopg2.extensions.POLL_WRITE:\n",
    "            select.select([], [conn.fileno()], [])\n",
    "        elif state == psycopg2.extensions.POLL_READ:\n",
    "            select.select([conn.fileno()], [], [])\n",
    "        else:\n",
    "            raise psycopg2.OperationalError(\"poll() returned %s\" % state)\n",
    "\n",
    "def get_status():\n",
    "    status_sql = \"\"\"\n",
    "    SELECT c.relname as tablename, s.*, a.query FROM pg_stat_progress_create_index s \n",
    "    JOIN pg_stat_activity a ON s.pid = a.pid\n",
    "    JOIN pg_class c on s.relid = c.oid\n",
    "    WHERE query LIKE '%-- amsterdamumcdb indices%';\n",
    "    \"\"\"\n",
    "    status = pd.read_sql(status_sql, con)\n",
    "    if len(status) > 0:\n",
    "        pid = status['pid'][0]\n",
    "        phase = status['phase'][0]\n",
    "        tablename = status['tablename'][0]\n",
    "        \n",
    "        if 'tuples' in phase:\n",
    "            total = status['tuples_total'][0]\n",
    "            current = status['tuples_done'][0]\n",
    "        else:\n",
    "            total = status['blocks_total'][0]\n",
    "            current = status['blocks_done'][0]\n",
    "\n",
    "        return pid, total, current, phase, tablename\n",
    "    else:\n",
    "        #no running index available\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "\n",
    "#create a new async connection, to monitor the status from another connection while the indexing is running.\n",
    "aconn = psycopg2.connect(database=config['psycopg2']['database'], \n",
    "                       user=config['psycopg2']['username'], password=config['psycopg2']['password'], \n",
    "                       host=config['psycopg2']['host'], port=config['psycopg2']['port'], async_=1) \n",
    "\n",
    "wait(aconn) #wait until connection is ready\n",
    "acurs = aconn.cursor()\n",
    "\n",
    "index_sql = \"\"\"\n",
    "-- amsterdamumcdb indices\n",
    "SET SCHEMA 'amsterdamumcdb';\n",
    "-- admissions table\n",
    "CREATE INDEX admissions_admissionid_index ON admissions (admissionid);\n",
    "CREATE INDEX admissions_patientid_index ON admissions (patientid);\n",
    "\n",
    "-- drugitems table\n",
    "CREATE INDEX drugitems_admissionid_index ON drugitems (admissionid);\n",
    "CREATE INDEX drugitems_orderid_index ON drugitems (orderid);\n",
    "CREATE INDEX drugitems_ordercategoryid_index ON drugitems (ordercategoryid);\n",
    "CREATE INDEX drugitems_itemid_index ON drugitems (itemid);\n",
    "CREATE INDEX drugitems_start_index ON drugitems (start);\n",
    "CREATE INDEX drugitems_stop_index ON drugitems (stop);\n",
    "\n",
    "-- freetextitems table\n",
    "CREATE INDEX freetextitems_admissionid_index ON freetextitems (admissionid);\n",
    "CREATE INDEX freetextitems_itemid_index ON freetextitems (itemid);\n",
    "CREATE INDEX freetextitems_measuredat_index ON freetextitems (measuredat);\n",
    "\n",
    "-- listitems table\n",
    "CREATE INDEX listitems_admissionid_index ON listitems (admissionid);\n",
    "CREATE INDEX listitems_itemid_index ON listitems (itemid);\n",
    "CREATE INDEX listitems_measuredat_index ON listitems (measuredat);\n",
    "\n",
    "-- numericitems table\n",
    "CREATE INDEX numericitems_admissionid_index ON numericitems (admissionid);\n",
    "CREATE INDEX numericitems_itemid_index ON numericitems (itemid);\n",
    "CREATE INDEX numericitems_measuredat_index ON numericitems (measuredat);\n",
    "CREATE INDEX numericitems_admission_item_time_index ON numericitems (admissionid, itemid, measuredat);\n",
    "CREATE INDEX numericitems_islabresult_index ON numericitems (islabresult);\n",
    "CREATE INDEX numericitems_fluidout_index ON numericitems (fluidout);\n",
    "\n",
    "-- procedureorderitems table\n",
    "CREATE INDEX procedureorderitems_admissionid_index ON procedureorderitems (admissionid);\n",
    "CREATE INDEX procedureorderitems_itemid_index ON procedureorderitems (itemid);\n",
    "CREATE INDEX procedureorderitems_ordercategoryid_index ON procedureorderitems (ordercategoryid);\n",
    "CREATE INDEX procedureorderitems_registeredat_index ON procedureorderitems (registeredat);\n",
    "\n",
    "-- processitems table\n",
    "CREATE INDEX processitems_admissionid_index ON processitems (admissionid);\n",
    "CREATE INDEX processitems_itemid_index ON processitems (itemid);\n",
    "CREATE INDEX processitems_start_index ON processitems (start);\n",
    "CREATE INDEX processitems_stop_index ON processitems (stop);\n",
    "\"\"\"\n",
    "#execute create index sql query **asychronously**\n",
    "acurs.execute(index_sql)\n",
    "\n",
    "#wait 2 seconds to allow for starting up the indexing process \n",
    "time.sleep(2)\n",
    "\n",
    "#progress bar variables\n",
    "progress_blocks = None\n",
    "progress_tuples = None\n",
    "progress_current = None\n",
    "\n",
    "#used for progress of indexing the database (based on indices in release 1.0.1)\n",
    "TOTAL_INDEX_BLOCKS = 98208095\n",
    "TOTAL_INDEX_TUPLES = 6.006120e+09\n",
    "\n",
    "#get the current status of the indexing operation\n",
    "\n",
    "pid, total, current, previous_phase, previous_tablename = get_status()\n",
    "if pid == None:\n",
    "    print('No indexing in progress.')\n",
    "else:\n",
    "\n",
    "    #create two tqdm objects for progress bars\n",
    "    progress_blocks = tqdm(total=TOTAL_INDEX_BLOCKS, desc='Total blocks read', dynamic_ncols=True, \n",
    "                           unit_scale=1, unit='blocks') #total blocks progress\n",
    "    progress_tuples = tqdm(total=TOTAL_INDEX_TUPLES, desc='Total tuples written', dynamic_ncols=True, \n",
    "                           unit_scale=1, unit='tuples') #total tuples progress\n",
    "    progress_current = tqdm(total=total, desc='Processing table ' + previous_tablename + ' (' + previous_phase + ')',\n",
    "                            dynamic_ncols=True, unit_scale=1, leave=False) #current index progress\n",
    "\n",
    "    #update the progress bars every two seconds until the progress query returns empty or changes\n",
    "    while True:  \n",
    "            current_pid, total, current, phase, tablename = get_status()\n",
    "\n",
    "            if pid == None or current_pid != pid:\n",
    "                break #process changed: done\n",
    "\n",
    "            #phase changed, if 'scanning table' a new item was started\n",
    "            if (not previous_phase == phase) or \\\n",
    "                (not previous_tablename == tablename):\n",
    "                \n",
    "                reset = False\n",
    "                if 'scanning table' in phase: #scanning tables, loading tuples in tree\n",
    "                    #previous phase ('tuples') should be added completely to progress bar\n",
    "                    progress_tuples.update(progress_current.total - progress_current.n)\n",
    "                    reset = True\n",
    "                    \n",
    "                elif 'loading tuples in tree' in phase: #loading tuples in tree\n",
    "                    #previous phase ('scanning table') should be added completely to progress bar\n",
    "                    progress_blocks.update(progress_current.total - progress_current.n)          \n",
    "                    reset = True\n",
    "                \n",
    "                if reset:\n",
    "                    #finalize (100%) the progress bar, remove it and start a new one\n",
    "                    progress_current.n = progress_current.total\n",
    "                    progress_current.close()\n",
    "                    progress_current = tqdm(total=total, desc='Processing table ' + tablename + ' (' + phase + ')',\n",
    "                                            dynamic_ncols=True, unit_scale=1, leave=False) #current index progress\n",
    "            \n",
    "            #update progress bars based on the phase\n",
    "            update = False\n",
    "            if 'scanning table' in phase: #scanning tables, loading tuples in tree\n",
    "\n",
    "                progress_current.unit = 'blocks'\n",
    "\n",
    "                #update the progress of total blocks to read\n",
    "                progress_blocks.update(current - progress_current.n)\n",
    "                update = True\n",
    "\n",
    "            elif 'loading tuples in tree' in phase:\n",
    "                progress_current.unit = 'tuples'\n",
    "\n",
    "                #update the progress of total tuples written\n",
    "                progress_tuples.update(current - progress_current.n)\n",
    "                update = True\n",
    "\n",
    "            if update:\n",
    "                #update the progress of the current index\n",
    "                progress_current.n = current\n",
    "                progress_current.update(0) #trigger refresh\n",
    "\n",
    "                #store the current phase to compare with the next\n",
    "                previous_phase = phase\n",
    "                previous_tablename = tablename\n",
    "\n",
    "            \n",
    "            time.sleep(1) #wait for one second before requerying indexing progress\n",
    "\n",
    "    #left while loop\n",
    "    if not progress_blocks is None:\n",
    "        progress_blocks.n = TOTAL_INDEX_BLOCKS\n",
    "        progress_blocks.close()\n",
    "    if not progress_tuples is None:\n",
    "        progress_tuples.n = TOTAL_INDEX_TUPLES\n",
    "        progress_tuples.close()\n",
    "    if not progress_current is None:\n",
    "        progress_current.close()\n",
    "        \n",
    "    print('Indexing done.')\n",
    "    \n",
    "wait(aconn) #wait until connection is ready\n",
    "aconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
